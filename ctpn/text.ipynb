{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import torch\n",
    "import torchvision\n",
    "import torch.nn as nn\n",
    "from torchvision.models.detection.transform import GeneralizedRCNNTransform\n",
    "from torchvision.models.detection.backbone_utils import resnet_fpn_backbone"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 图像数据\n",
    "img = cv2.imread('1.jpg')\n",
    "# label数据，好像有几个类\n",
    "label = {\n",
    "    \"boxes\":[[100, 100, 200, 300]],\n",
    "    \"label\": 1,\n",
    "    \"image_id\": 2\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "torch.Size([1, 3, 333, 500])"
     },
     "metadata": {},
     "execution_count": 3
    }
   ],
   "source": [
    "img = []\n",
    "img.append(cv2.imread('1.jpg'))\n",
    "img = torch.LongTensor(img).permute(0, 3, 1, 2).float()\n",
    "img.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "5\n"
    }
   ],
   "source": [
    "backbone = resnet_fpn_backbone('resnet50', False)\n",
    "output = backbone(img)\n",
    "print(len(output))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class RPNhead(nn.Module):\n",
    "\n",
    "    def __init__(self, in_channels, num_anchors):\n",
    "        super(RPNhead, self).__init__()\n",
    "        self.conv = nn.Conv2d(\n",
    "            in_channels, in_channels, kernel_size=3, stride=1, padding=1\n",
    "        )\n",
    "        self.cls_logits = nn.Conv2d(\n",
    "            in_channels, num_anchors, kernel_size=1, stride=1\n",
    "        )\n",
    "        self.bbox_pred = nn.Conv2d(\n",
    "            in_channels, num_anchors * 4, kernel_size=1, stride=1\n",
    "        )\n",
    "\n",
    "        # init parameters\n",
    "        for l in self.children():\n",
    "            torch.nn.init.normal_(l.weight, std=0.01)\n",
    "            torch.nn.init.constant_(l.bias, 0)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # 输入的是feature_map, feature_map有可能是多层(resnet)\n",
    "        # x: C * [batch_size, out_channel, H_out, W_out]\n",
    "        features = list(x.values())\n",
    "        logits = []\n",
    "        bbox_reg = []\n",
    "        for feature in features:\n",
    "            t = nn.functional.relu(self.conv(feature))\n",
    "            logits.append(self.cls_logits(t))\n",
    "            bbox_reg.append(self.bbox_pred(t))\n",
    "        return bbox_reg, logits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_channel = backbone.out_channels\n",
    "Anchor_sizes = (16, 64, 128)\n",
    "aspect_ratios = (0.5, 1, 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "rpn_head = RPNhead(input_channel, len(Anchor_sizes) * len(aspect_ratios))\n",
    "bbox_reg, logits = rpn_head(output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "[torch.Size([1, 256, 84, 125]),\n torch.Size([1, 256, 42, 63]),\n torch.Size([1, 256, 21, 32]),\n torch.Size([1, 256, 11, 16]),\n torch.Size([1, 256, 6, 8])]"
     },
     "metadata": {},
     "execution_count": 8
    }
   ],
   "source": [
    "# 生成anchor的各个中心点\n",
    "# 已知feature_map有5层\n",
    "[i.shape for i in output.values()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 获取每层feature_map的尺寸\n",
    "feature_size = [i.shape[-2:] for i in output.values()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 获取输入图片的尺寸\n",
    "img_size = img.shape[-2:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "[torch.Size([84, 125]), torch.Size([42, 63]), torch.Size([21, 32]), torch.Size([11, 16]), torch.Size([6, 8])]\ntorch.Size([333, 500])\n"
    }
   ],
   "source": [
    "print(feature_size)\n",
    "print(img_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "#计算每层的stride\n",
    "stride = torch.LongTensor([[img_size[0]//f[0], img_size[1]//f[1]] for f in feature_size])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "tensor([[ 3,  4],\n        [ 7,  7],\n        [15, 15],\n        [30, 31],\n        [55, 62]])\n"
    }
   ],
   "source": [
    "print(stride)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "output_type": "error",
     "ename": "NameError",
     "evalue": "name 'base_anchor' is not defined",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-16-04973beffdd7>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      8\u001b[0m     \u001b[0mx\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mx\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mreshape\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m-\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      9\u001b[0m     \u001b[0mcenter\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mstack\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0my\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0my\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdim\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 10\u001b[1;33m     \u001b[0mcenters\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcenter\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mview\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m-\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m4\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m+\u001b[0m \u001b[0mbase_anchor\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mview\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m-\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m4\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mview\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m-\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;36m4\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m: name 'base_anchor' is not defined"
     ]
    }
   ],
   "source": [
    "# 这样的话我们就可以生成,每个anchor的中心点\n",
    "centers = []\n",
    "for f, s in zip(feature_size, stride):\n",
    "    y = torch.arange(0, f[0], dtype = torch.float32) * s[0]\n",
    "    x = torch.arange(0, f[1], dtype = torch.float32) * s[1]\n",
    "    y, x = torch.meshgrid(y,x)\n",
    "    y = y.reshape(-1)\n",
    "    x = x.reshape(-1)\n",
    "    center = torch.stack((y,x,y,x), dim = 1)\n",
    "    centers.append((center.view(-1, 1, 4) + base_anchor.view(1, -1, 4)).view(-1,4))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 183,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[torch.Size([94500, 4]),\n",
       " torch.Size([23814, 4]),\n",
       " torch.Size([6048, 4]),\n",
       " torch.Size([1584, 4]),\n",
       " torch.Size([432, 4])]"
      ]
     },
     "execution_count": 183,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(len(centers))\n",
    "[i.shape for i in centers]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 生成每个anchor的长宽\n",
    "# Anchor_sizes = (16, 64, 128)\n",
    "# aspect_ratios = (0.5, 1, 2)\n",
    "Anchor_scales = torch.as_tensor(Anchor_sizes, dtype = torch.float32)\n",
    "aspect_ratios = torch.as_tensor(aspect_ratios, dtype = torch.float32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 173,
   "metadata": {},
   "outputs": [],
   "source": [
    "h_ratios = torch.sqrt(aspect_ratios)\n",
    "w_ratios = 1/h_ratios\n",
    "\n",
    "ws = (h_ratios[:, None] * Anchor_scales[None, :]).view(-1)\n",
    "hs = (w_ratios[:, None] * Anchor_scales[None, :]).view(-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 174,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([9])"
      ]
     },
     "execution_count": 174,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ws.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 179,
   "metadata": {},
   "outputs": [],
   "source": [
    "base_anchor = (torch.stack([-ws, -hs, ws, hs], dim = 1)/2).round()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 176,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([9, 4])"
      ]
     },
     "execution_count": 176,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "base_anchor.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 177,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[ -5.6569, -11.3137,   5.6569,  11.3137],\n",
       "         [-22.6274, -45.2548,  22.6274,  45.2548],\n",
       "         [-45.2548, -90.5097,  45.2548,  90.5097],\n",
       "         [ -8.0000,  -8.0000,   8.0000,   8.0000],\n",
       "         [-32.0000, -32.0000,  32.0000,  32.0000],\n",
       "         [-64.0000, -64.0000,  64.0000,  64.0000],\n",
       "         [-11.3137,  -5.6569,  11.3137,   5.6569],\n",
       "         [-45.2548, -22.6274,  45.2548,  22.6274],\n",
       "         [-90.5097, -45.2548,  90.5097,  45.2548]]])"
      ]
     },
     "execution_count": 177,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "base_anchor.view(1, -1, 4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 164,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([3, 4, 3])"
      ]
     },
     "execution_count": 164,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "base_anchor.round()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Anchor_generator(nn.Module):\n",
    "\n",
    "    def __init__(self, Anchor_sizes = (64, 256, 512), aspect_ratios = (0.5, 1.0, 2.0)):\n",
    "        super(Anchor_generator, self).__init__()\n",
    "\n",
    "        self.Anchor_sizes = Anchor_sizes\n",
    "        self.aspect_ratios = aspect_ratios\n",
    "\n",
    "    # 生成Anchor的话，需要原输入图片的img跟feature_map\n",
    "    def forward(self, imgs, feature_maps):\n",
    "        # imgs [batch_size, 3, H, W]\n",
    "        # feature_maps C * [batch_size, out_channel, h_out, w_out]\n",
    "\n",
    "        # 获取图片的尺寸\n",
    "        img_sizes = imgs.shape[-2:]\n",
    "        # 获取每一层feature_map的尺寸\n",
    "        features_size = tuple([feature_map.shape[-2:] for feature_map in feature_maps])\n",
    "        # 计算一个feature_map的特征所代表图像的像素点个数\n",
    "        stride = [[img_sizes[0] / f[0], img_sizes[1] / f[1]] for f in features_size]\n",
    "\n",
    "        # 生成base_anchor\n",
    "        Anchor_sizes = torch.as_tensor(self.Anchor_sizes, dtype=torch.float32)\n",
    "        aspect_ratios = torch.as_tensor(self.aspect_ratios, dtype=torch.float32)\n",
    "\n",
    "        h_ratios = torch.sqrt(aspect_ratios)\n",
    "        w_ratios = 1 / h_ratios\n",
    "\n",
    "        # 所有ratio对各个Anchor_size做计算\n",
    "        hs = (h_ratios[:, None] * Anchor_sizes[None, :]).view(-1)\n",
    "        ws = (w_ratios[:, None] * Anchor_sizes[None, :]).view(-1)\n",
    "\n",
    "        # hs, ws做拼接 N * 4, round 取整\n",
    "        base_anchor = (torch.stack([-hs, -ws, hs, ws], dim=1) / 2).round()\n",
    "\n",
    "        # 生成每个feature_map层的中心点, 获取每层的尺寸和步长stride\n",
    "        anchors = []\n",
    "        for f, s in zip(features_size, stride):\n",
    "            # 生成h_c, w_c点\n",
    "            feature_h, feature_w = f\n",
    "            stride_h, stride_w = s\n",
    "            h_c = torch.arange(0, feature_h, dtype=torch.float32) * stride_h\n",
    "            w_c = torch.arange(0, feature_w, dtype=torch.float32) * stride_w\n",
    "            h_c, w_c = torch.meshgrid(h_c, w_c)\n",
    "            h_c = h_c.reshape(-1)\n",
    "            w_c = w_c.reshape(-1)\n",
    "            anchor = torch.stack([h_c, w_c, h_c, w_c]).view(-1, 1, 4) + \\\n",
    "                     base_anchor.view(1, -1, 4)\n",
    "\n",
    "            anchors.append(anchor.view(-1, 4))\n",
    "        return anchors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "anchor_gener = Anchor_generator()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_maps = [i for i in output.values()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "anchors = anchor_gener(img, output.values())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "torch.Size([94500, 4])"
     },
     "metadata": {},
     "execution_count": 23
    }
   ],
   "source": [
    "anchors[0].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 209,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[torch.Size([94500, 4]),\n",
       " torch.Size([23814, 4]),\n",
       " torch.Size([6048, 4]),\n",
       " torch.Size([1584, 4]),\n",
       " torch.Size([432, 4])]"
      ]
     },
     "execution_count": 209,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "[anchor.shape for anchor in anchors]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "layer = backbone[:-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Sequential(\n",
       "  (0): Sequential(\n",
       "    (0): Conv2d(3, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (1): ReLU(inplace=True)\n",
       "    (2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (3): ReLU(inplace=True)\n",
       "    (4): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "    (5): Conv2d(64, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (6): ReLU(inplace=True)\n",
       "    (7): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (8): ReLU(inplace=True)\n",
       "    (9): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "    (10): Conv2d(128, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (11): ReLU(inplace=True)\n",
       "    (12): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (13): ReLU(inplace=True)\n",
       "    (14): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (15): ReLU(inplace=True)\n",
       "    (16): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "    (17): Conv2d(256, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (18): ReLU(inplace=True)\n",
       "    (19): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (20): ReLU(inplace=True)\n",
       "    (21): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (22): ReLU(inplace=True)\n",
       "    (23): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "    (24): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (25): ReLU(inplace=True)\n",
       "    (26): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (27): ReLU(inplace=True)\n",
       "    (28): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (29): ReLU(inplace=True)\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.nn.Sequential(layer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "file_extension": ".py",
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4-final"
  },
  "mimetype": "text/x-python",
  "name": "python",
  "npconvert_exporter": "python",
  "pygments_lexer": "ipython3",
  "version": 3
 },
 "nbformat": 4,
 "nbformat_minor": 2
}